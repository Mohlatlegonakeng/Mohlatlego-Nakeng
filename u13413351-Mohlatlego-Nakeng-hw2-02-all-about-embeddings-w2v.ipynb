{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Mohlatlego Nakeng\"\n",
    "StudentNumber = \"13413351\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45f2081148e44cbfd58111be85c4041c",
     "grade": false,
     "grade_id": "cell-7dde6d5d97a365d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## COS 802 2019 Homework 2 - Part 3 [35 points, 42 with extra credit]\n",
    "## Exploring Word Embeddings.\n",
    "\n",
    "\n",
    "**You will learn how to:**\n",
    "- Train your own word embedding.\n",
    "- Use pretrained word embeddings.\n",
    "- Use word embeddings in simple classifiers\n",
    "\n",
    "**What this homework does not cover!!!!**\n",
    "- Sequential Deep Learning Models with embeddings - A challenge you have with words is that the context is defined by the sequential context. [Resource 1](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) [Resource 2](https://www.coursera.org/lecture/nlp-sequence-models/why-sequence-models-0h7gT)\n",
    "- Other Deep learning models with embeddings [Resource 3](https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn)\n",
    "\n",
    "**Note:** you can experiment by adding aditional cells, but they must be removed from final solution. Only the cells originally in the notebook plus the ones you have filled with your solution are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3b2f6a606a52c95e4204c03c3472602",
     "grade": false,
     "grade_id": "cell-289a4ce55ad09b5a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1 Packages ##\n",
    "\n",
    "Scikit-Learn for text Analysis\n",
    "- [sklearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html): Scikit-Learn Working With Text Data\n",
    "- [matplotlib](http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- [gensim](https://pypi.org/project/gensim/) NLP library with word-embedding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from wordcloud) (6.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from wordcloud) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from wordcloud) (1.18.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.15.2)\n",
      "Requirement already satisfied: boto in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.2 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.18.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from botocore<1.19.0,>=1.18.2->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c170edb1600cc519e60d6f1b52b01e6",
     "grade": false,
     "grade_id": "cell-bda822d570cf9afc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160e8442239e725b02a0abf1291baa29",
     "grade": false,
     "grade_id": "cell-52a82cff469136e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Fetch the AGNews dataset from the FastAI repository\n",
    "\n",
    "Fast AI repo https://course.fast.ai/datasets\n",
    "\n",
    "> 496,835 categorized news articles from >2000 news sources from the 4 largest classes from AG’s corpus of news articles, using only the title and description fields. The number of training samples for each class is 30,000 and testing 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile('ag_news_csv.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz\n",
    "\n",
    "#U ncompress the archive\n",
    "if not os.path.isfile('ag_news_csv/train.csv'):\n",
    "    !tar -xzf ag_news_csv.tgz\n",
    "    # Lets see what is in the \n",
    "    !ls ag_news_csv/\n",
    "# View categories\n",
    "!cat ag_news_csv/classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0         3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1         3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2         3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3         3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4         3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ag_news_train  = pd.read_csv('ag_news_csv/train.csv', header = None)\n",
    "df_ag_news_train.columns = ['category','title','text']\n",
    "df_ag_news_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Building own Word2Vec [4 points]\n",
    "### What is word2vec\n",
    "\n",
    "*Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.* [URL](https://en.wikipedia.org/wiki/Word2vec)\n",
    "\n",
    "**Task:**\n",
    "\n",
    "You will use gensim to be able to train both skip-gram and cbow word2vec models using the AGNews data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "153fd5c1035c76b877b77df4d7cee56f",
     "grade": false,
     "grade_id": "cell-e0d9e6e34140f0ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original doc:  Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Tokenized doc:  ['reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'s\", 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n",
      "Num Sentences:  154351\n"
     ]
    }
   ],
   "source": [
    "documents = df_ag_news_train.text.values\n",
    "\n",
    "# Go through all the documents and tokenize them by words\n",
    "data_gensim = []\n",
    "for doc in documents:\n",
    "    \n",
    "    # We are training on sentences\n",
    "    for sentence in sent_tokenize(doc):\n",
    "        tokens = []\n",
    "        # We want only lower case\n",
    "        for word in word_tokenize(sentence):\n",
    "            tokens.append(word.lower())\n",
    "        data_gensim .append(tokens)\n",
    "\n",
    "# The result\n",
    "print(\"Original doc: \", documents[0])\n",
    "print(\"Tokenized doc: \", data_gensim[0])\n",
    "print(\"Num Sentences: \", len(data_gensim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51a25b4078388d43ed6afe6506a07fde",
     "grade": false,
     "grade_id": "cell-323f39eb56795929",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hmm, that does not seem right. Can we remove all of the special characters? Let's use the RegexTokenizer that can use a regular expression that removes special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # Can you figure out what the regular expression is doing here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original doc:  Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Tokenized doc:  ['reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again']\n",
      "Num Sentences:  154351\n"
     ]
    }
   ],
   "source": [
    "documents = df_ag_news_train.text.values\n",
    "\n",
    "# Go through all the documents and tokenize them by words\n",
    "data_gensim = []\n",
    "for doc in documents:\n",
    "    # We are training on sentences\n",
    "    for sentence in sent_tokenize(doc):\n",
    "        tokens = []\n",
    "        # We want only lower case\n",
    "        for word in tokenizer.tokenize(sentence): # We are using the new tokeniser here\n",
    "            tokens.append(word.lower())\n",
    "        data_gensim .append(tokens)\n",
    "\n",
    "# The result\n",
    "print(\"Original doc: \", documents[0])\n",
    "print(\"Tokenized doc: \", data_gensim[0])\n",
    "print(\"Num Sentences: \", len(data_gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec properties\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_count_words = 1 # minimum times we must see a word before we make a vector for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 \n",
    "\n",
    "Take a look at **[gensim.models.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)** and train a **CBOW** model. The model should be saved in *model_cbow*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "008bbb0bd4b0149e8b204df3379705a7",
     "grade": true,
     "grade_id": "cell-5ee93336cf9537aa",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "# YOUR CODE HERE\n",
    "from gensim.models import Word2Vec\n",
    "model_cbow = Word2Vec( data_gensim, min_count=min_count_words,size= embedding_size, window= window_size)\n",
    "\n",
    "# Note: This will take some time. Be patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ada7a6b73323bacaf3e218bfdfe51e4",
     "grade": false,
     "grade_id": "cell-76adb97d6efe51d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lets see a vector for the word *money*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4268e0b1cba28cb9051cf51b5bf4b6",
     "grade": false,
     "grade_id": "cell-472dfcd698c4ae85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43023917,  0.49391034, -0.29455504, -0.16713655, -0.7198631 ,\n",
       "        1.1437222 ,  0.33048776, -0.79975283,  0.16143873,  0.22538131,\n",
       "       -0.69949865,  0.42010462,  0.02072031,  0.27805907, -0.02141028,\n",
       "        0.01897099, -0.3570757 , -0.1552344 , -0.11092183, -0.10134619,\n",
       "       -0.89406645, -0.25430495, -0.7951853 ,  0.7433451 ,  0.37337   ,\n",
       "        0.13273127, -0.252723  ,  0.29027316,  0.49601826, -0.547723  ,\n",
       "        0.17721382, -0.06368242, -0.22159591, -0.55978185,  0.02974451,\n",
       "        0.78752035,  0.7518917 , -1.2577966 , -0.45998067,  0.03627203,\n",
       "       -0.8557342 , -0.34061185,  0.3619586 ,  0.19925684,  0.14900613,\n",
       "       -0.29531407, -0.09845301,  0.14405546, -0.13352919,  0.11729415,\n",
       "        0.3884275 ,  0.08272228,  0.40851918, -0.1461801 ,  0.0803031 ,\n",
       "        0.84643286,  0.504147  , -0.71561295, -0.8155103 ,  0.4897443 ,\n",
       "       -0.02334483,  0.36437902, -0.47386777, -0.54606193, -0.29890427,\n",
       "        0.34048876, -0.12005597,  0.5151365 ,  0.07837338, -0.5284299 ,\n",
       "       -0.49060205, -0.01413486, -0.35430062, -0.11140168, -0.02395579,\n",
       "       -0.8703851 , -0.30230168, -0.11728828,  0.17509805, -1.4582376 ,\n",
       "       -0.37274534, -0.8681929 , -0.47881523, -0.01051453, -0.327921  ,\n",
       "        0.5699242 , -0.54396665, -0.16722518, -0.6226244 , -0.6379893 ,\n",
       "        0.25943154,  0.24338435,  0.21078251,  0.5027304 ,  0.5574963 ,\n",
       "       -0.14045262, -1.3483301 ,  0.7371907 , -0.32434535, -0.9726993 ,\n",
       "        0.33676443,  0.4938695 ,  0.33706838, -0.66534716,  0.6611295 ,\n",
       "       -1.0804183 , -0.27776378,  0.06273726,  0.15900387,  0.7083483 ,\n",
       "       -0.6319374 , -0.37585098,  0.68731135, -0.00609656,  0.17923902,\n",
       "        0.5797324 ,  0.4298098 , -0.39138022,  0.55797315,  0.06833063,\n",
       "        0.795633  ,  0.03257817, -1.0024139 ,  0.76172936, -0.66534793,\n",
       "        0.8263389 ,  0.23034522,  0.56585056,  0.31294784, -0.13499188,\n",
       "        0.3392943 ,  0.6037922 ,  1.6727931 ,  0.6590997 , -0.7459103 ,\n",
       "        0.07455697,  0.3606873 ,  0.37029204, -0.7756119 , -0.11975862,\n",
       "        1.3796611 , -0.50624615, -0.46366957,  0.2887599 ,  0.03275737,\n",
       "        0.9821616 , -0.84336346,  0.11026327, -0.28254184,  0.09011655,\n",
       "        0.14559026,  0.04663264, -0.07944767, -0.12780412,  0.97679716,\n",
       "        0.70272386, -0.25287905, -0.00522365, -0.28465787, -0.40691328,\n",
       "       -0.04209401, -0.24607904, -0.7816019 , -0.30323994, -0.03682357,\n",
       "       -0.8683166 , -0.6055639 , -0.20810363, -0.04371167,  0.05159985,\n",
       "        0.22279663, -0.21315052,  0.2773476 , -0.10537533, -0.01749318,\n",
       "       -0.1637019 ,  1.8896286 ,  0.19615766, -0.18054964,  0.16696042,\n",
       "        0.12625599,  0.6876866 , -0.09470534, -0.62872493, -0.61904997,\n",
       "        0.23409481,  0.44695553, -0.42731205, -0.6207833 ,  0.23995297,\n",
       "        0.45008507, -0.6046917 ,  0.0244563 , -0.44920737, -0.03444347,\n",
       "       -0.860139  , -0.7047996 , -0.19314256, -0.10023878, -0.89700705,\n",
       "       -0.18836915, -0.58849704, -0.00422489, -0.6728173 , -0.21370514,\n",
       "        0.78924143, -0.22843537, -0.28076568, -1.0052387 , -0.24617709,\n",
       "        0.04906983, -0.584229  ,  0.17765005, -0.2266876 , -0.7423465 ,\n",
       "       -0.91506344, -0.46610516, -1.1269104 , -0.3067923 ,  0.12850964,\n",
       "       -0.8513559 ,  0.51539695,  0.02160526,  0.31247407,  0.26392028,\n",
       "       -0.09126126,  0.256234  ,  0.9890985 , -1.0522234 , -0.86525214,\n",
       "       -0.6020725 ,  0.5548933 ,  0.48943147,  0.6122858 ,  0.42986247,\n",
       "        0.13930503,  1.0768169 ,  0.69025075,  0.2867401 , -1.260581  ,\n",
       "       -0.09944922,  0.19284505, -0.6009089 , -0.367264  , -0.5696102 ,\n",
       "       -1.084679  ,  0.25263917, -0.10506583, -0.03157135, -1.2353728 ,\n",
       "        0.19698435,  0.672988  ,  0.31468004, -0.09781018, -0.21509981,\n",
       "       -0.4076319 ,  0.08769478,  0.27022773,  0.39687368, -0.9002919 ,\n",
       "        0.12619078, -0.24744034, -0.3328837 ,  0.6635745 ,  0.23582543,\n",
       "       -0.381093  ,  0.31149518, -0.06627221,  0.09165081,  0.03592142,\n",
       "        0.14676864,  0.3922805 , -0.17152122, -0.2651072 ,  0.02715678,\n",
       "        0.41685   , -0.42005932,  0.5329329 , -0.14462852, -0.9792833 ,\n",
       "       -0.33526468, -0.17694879,  0.9160737 , -0.31693175, -0.8161893 ,\n",
       "        0.36301795,  0.8537977 ,  0.7006447 , -0.04355677, -0.10905852,\n",
       "       -0.05691465, -0.14102818, -0.96213293,  0.20042892,  0.00753282,\n",
       "        0.0632972 , -0.80372345,  0.02304673,  0.5914766 ,  0.8189758 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow.wv['money']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we can do with word embeddings we could not do with bag-of-words models is that we can now do [cosine similarity](https://www.machinelearningplus.com/nlp/cosine-similarity/) of words. 1.0 is perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW similarity:  0.56732136\n"
     ]
    }
   ],
   "source": [
    "print(\"CBOW similarity: \", model_cbow.wv.similarity('money', 'cash'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Most Similar to Money\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('benefits', 0.6512506008148193),\n",
       " ('customers', 0.6380195021629333),\n",
       " ('paying', 0.6330016851425171),\n",
       " ('consumers', 0.6310704350471497),\n",
       " ('funds', 0.6260761022567749),\n",
       " ('savings', 0.6136110424995422),\n",
       " ('cowbird', 0.610594630241394),\n",
       " ('options', 0.6009640693664551),\n",
       " ('ways', 0.599640965461731),\n",
       " ('certain', 0.5945741534233093)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CBOW Most Similar to Money\")\n",
    "model_cbow.wv.most_similar('money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2\n",
    "\n",
    "Now, Take a look at **[gensim.models.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)** and train a **Skip-Gram** model. The model should be saved in *model_skipgram*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d17a6ff15f052b743e85fc07e066d25",
     "grade": true,
     "grade_id": "vectoriser_limit",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "# YOUR CODE HERE\n",
    "\n",
    "model_skipgram = Word2Vec( data_gensim, min_count=min_count_words,size= embedding_size, window= window_size, sg=1) \n",
    "#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "# Note: This will take some time. Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram similarity:  0.43862706\n"
     ]
    }
   ],
   "source": [
    "print(\"Skip-Gram similarity: \", model_skipgram.wv.similarity('money', 'cash'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Most Similar to Money\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('changers', 0.6007676720619202),\n",
       " ('recoup', 0.5959882140159607),\n",
       " ('payoffs', 0.5949201583862305),\n",
       " ('laundering', 0.579896092414856),\n",
       " ('gifts', 0.5775237083435059),\n",
       " ('inclined', 0.5665773153305054),\n",
       " ('deposit', 0.56183260679245),\n",
       " ('portfolios', 0.5608170032501221),\n",
       " ('parija', 0.5567904710769653),\n",
       " ('taxpayer', 0.556017279624939)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"SkipGram Most Similar to Money\")\n",
    "model_skipgram.wv.most_similar('money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6c85ad4c1a2bd0ec5ec7732f153ab74",
     "grade": false,
     "grade_id": "cell-bb618a853c2ba264",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Pretrained Embeddings. [4 Points]\n",
    "\n",
    "A great feature of embeddings is someone else can pretrain them for us. In this case we can get the original [Google News word2vec vectors](https://code.google.com/archive/p/word2vec/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gunzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# If you have not downloaded this, uncomment the next 2 lines\n",
    "!wget -c https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "705927136ff2b5a9c85a4d9da220ca3e",
     "grade": false,
     "grade_id": "cell-0da0cc935bcfabb8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "## Uncomment below\n",
    "\n",
    "# model_w2v_google = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "## Note: This will take some time. Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the cell above is not editable!\n",
    "#I was unable to access the files, instead i did manual download\n",
    "model_w2v_google = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment below\n",
    "#  print(\"Google News Pretrained W2V similarity: \", model_w2v_google.wv.similarity('money', 'cash'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment below\n",
    "\n",
    "print(\"Google News Pretrained W2V similarity: \", model_w2v_google.wv.most_similar('money'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment below\n",
    "\n",
    "print(\"Google Pretrained W2V Most Similar to Money\")\n",
    "model_w2v_google.wv.most_similar('money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa118efd342c5144232d79e8bd8eccb7",
     "grade": false,
     "grade_id": "cell-489880657c644c93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "**Q2.1:** Why do the 3 different word2vec models have differing distances between money and cash?\n",
    "\n",
    "**Q2.2:** Test other words and check if the distance makes sense to you. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b0924876119ba5fa805eb7042b9e30a",
     "grade": true,
     "grade_id": "cell-2564c509afb2f9b9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Answers\n",
    "\n",
    "A2.1 and A2.2 Here\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing Word Vectors with TSNE [4 points]\n",
    "In this section you will first reduce the large vector space (dimenstion 300) to a 2D space that you can then visualise the words and their semantic relations on a plot. We use tSNE for this.\n",
    "\n",
    "*t-distributed Stochastic Neighbor Embedding. t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.* [URL](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get data from the CBOW model so that we can then train the tSNE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "c:\\users\\mohlatlegon\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "labels_viz = []\n",
    "tokens_viz = []\n",
    "count = 0\n",
    "for word in model_cbow.wv.vocab:\n",
    "    if word in model_w2v_google.wv.vocab:\n",
    "        tokens_viz.append(model_cbow[word])\n",
    "        labels_viz.append(word)\n",
    "        if count > 10000:\n",
    "            break\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the TSNE Model for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_cbow_tsne = tsne_model.fit_transform(tokens_viz)\n",
    "# Note: This will take some time. Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = []\n",
    "y = []\n",
    "for value in values_cbow_tsne:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(20, 20)) \n",
    "for i in range(200):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels_viz[i],\n",
    "                    xy=(x[i], y[i]),\n",
    "                    xytext=(5, 2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same words in the gogogle model\n",
    "tokens_viz_w2v = []\n",
    "for word in labels_viz:\n",
    "        tokens_viz_w2v.append( model_w2v_google[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_w2v_tsne = tsne_model.fit_transform(tokens_viz_w2v)\n",
    "# Note: This will take some time. Be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = []\n",
    "y = []\n",
    "for value in values_w2v_tsne:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(20, 20)) \n",
    "for i in range(200):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels_viz[i],\n",
    "                    xy=(x[i], y[i]),\n",
    "                    xytext=(5, 2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3db9db3a5dae60543b06858df38d0b1",
     "grade": false,
     "grade_id": "cell-7326f94ddc304d55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 3:\n",
    "What do you observe in the two visualisations? Any similarities, what are the differences. What can you say about one embedding over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fecd14108db28f5cbd40e13d9976231",
     "grade": true,
     "grade_id": "topic_observations",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "### A3\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Classifiers and embeddings [40 points]\n",
    "\n",
    "In this task, which takes up most of this homework, you will experiment with different ways to use word embeddings to transform documents. For this section you will transform documents [multiple word embeddings] into a single vector and looking at the effect on classifier performance.\n",
    "\n",
    "* **Q1:** experiment with using a *sum* to transform the word embedding sequence to a document vector [5]\n",
    "* **Q2:** experiment with using a *max* to transform the word embedding sequence to a document vector [5]\n",
    "* **Q3:** experiment with using a 1 and 2 and also playing with the different CBOW, and SkipGram models to transform the word embedding sequence to a document vector [7]\n",
    "* **Q4: Discusion:** Discuss your observations on the performance results [10]\n",
    "* **Q5 [Extra Credit - extra 5 points]:** experiment with using approaches from [this paper](https://arxiv.org/abs/1803.01400) to transform the word embedding sequence to a document vector - Paper: *Concatenated Power Mean Word Embeddings as Universal Cross-Lingual Sentence Representations*\n",
    "\n",
    "**Evalution:** Evaluate the performance via accuracy on predicting the document class. Example is shown below for a mean transform with logistic regression model.\n",
    "\n",
    "**Note** We will not cover using paragraph2vec or doc2vec but you can look at that yourself. It is available in the gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dca20c074143f9ddf1eb20cfa7fb9f76",
     "grade": true,
     "grade_id": "cell-d497323b29284fc7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to transform a document into a mean of the word vectors. \n",
    "\n",
    "# Note!! You should add your code in this function to implement Q1-5\n",
    "def embedding_transform(documents_tokenized, w2v_model, embedding_size, transform_type='mean'):\n",
    "    transformed_docs = []\n",
    "    for words in documents_tokenized:\n",
    "        transformed_doc = []\n",
    "        for w in words:\n",
    "            if w in w2v_model:\n",
    "                transformed_doc.append(w2v_model[w])\n",
    "            else:\n",
    "                transformed_doc.append(np.zeros(embedding_size)) ## Add a default 0 vector for unknown words\n",
    "        \n",
    "        # How do you transform to a document? Put your answer in here. \n",
    "        if  transform_type == 'mean':\n",
    "            transformed_doc = np.mean(transformed_doc, axis=0)\n",
    "        elif transform_type == 'sum':\n",
    "            # YOUR CODE HERE\n",
    "            transformed_doc = np.sum(transformed_doc, axis=0)\n",
    "        elif transform_type == 'max':\n",
    "            # YOUR CODE HERE\n",
    "            transformed_doc = np.max(transformed_doc, axis=0)\n",
    "        elif transform_type == 'extra':\n",
    "            # YOUR CODE HERE\n",
    "            transformed_doc = np.power(np.mean(np.power(transformed_doc,2),axis =0),1/2)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        transformed_docs.append(transformed_doc)\n",
    "    output = np.array(transformed_docs)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the documents and tokenize them by words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_tokenized = []\n",
    "for doc in documents:\n",
    "    tokens = []\n",
    "    # We want only lower case\n",
    "    for word in tokenizer.tokenize(doc): # We are using the new tokeniser here\n",
    "        tokens.append(word.lower())\n",
    "    documents_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ag_news_train.category # Get all the document labels\n",
    "\n",
    "# Now lets split into training and testing.\n",
    "doc_train, doc_test, y_train, y_test = train_test_split(documents_tokenized,y , test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we transform the data with the mean transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = embedding_transform(doc_train, model_cbow.wv, embedding_size, transform_type='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the size of each document is now the embedding_size. As you can see, every document is now only of size 300, instead of the larger sives with Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and then check score.\n",
    "clf = LogisticRegression(n_jobs=-1) # Use multicore\n",
    "scores = cross_val_score(clf, X_train_mean, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to all Task 4 questions below\n",
    "\n",
    "Do your experimentation below to answer the questions. You can add code lines, visualisations and tests as needed and also include the a **discussion**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cceb0e27ffd4edc76e0be9e08736a3d0",
     "grade": true,
     "grade_id": "cell-afc6ba14ca1359af",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_sum = embedding_transform(doc_train, model_cbow.wv, embedding_size, transform_type='sum')\n",
    "X_train_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f03b44550c8476e1ae83021ec9b8b535",
     "grade": true,
     "grade_id": "cell-56dccbb2b87be4a4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_max = embedding_transform(doc_train, model_cbow.wv, embedding_size, transform_type='max')\n",
    "X_train_max.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb12e3ebdadeeaf2c28ee324b91643d5",
     "grade": true,
     "grade_id": "cell-4b33cca9da5de5d6",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_sum_sg = embedding_transform(doc_train, model_skipgram.wv, embedding_size, transform_type='sum')\n",
    "X_train_sum_sg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 4.5 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b5e551ec527e132480baba0d8851c8f",
     "grade": true,
     "grade_id": "cell-9cf2d450fe764b2c",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train_extra = embedding_transform(doc_train, model_cbow.wv, embedding_size, transform_type='extra')\n",
    "X_train_extra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18aac9517bafba899209dfc5c9fa156b",
     "grade": true,
     "grade_id": "cell-913d24c76628c8a5",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Answer\n",
    "# YOUR CODE HERE\n",
    "The Skip-Gram algorithm performs better than the continous bag of words (CBOW) algorithm for the model.\n",
    "The Skip-Gram model had a better model accuracy (validation accuracy scores) across all the embedding transformations and the models ran quicker than the\n",
    "CBOW models.\n",
    "The different embedding transfomations are discussed below:\n",
    "Sum:\n",
    "Model Average Accuracy: CBOW: 87.6% and Skip-Gram: 88.9%\n",
    "Max:\n",
    "Model Average Accuracy: CBOW: 81.3% and Skip-Gram: 85.3%\n",
    "Mean:\n",
    "Model Average Accuracy: CBOW: 86.9% and Skip-Gram: 88.8%\n",
    "Power-Mean Algorthim:\n",
    "Model Average Accuracy: CBOW: 86.1% and Skip-Gram: 88.1%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
